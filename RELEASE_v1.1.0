HPX-5 Release notes, Version 1.1.0
----------------------------------
Release Date: 07/14/2015
Distributed under the Indiana University License. (See accompanying file
COPYING.txt) 

Product brief
--------------

HPX-5 (High Performance ParalleX) provides a unified programming model for
parallel and distributed applications. It is freely available, open source,
feature complete, and performance-oriented implementation of the ParalleX
execution model. HPX-5 is a general-purpose C runtime system for applications,
targeted at conventional, widely available architectures. We use HPX-5 for a
broad range of scientific applications, helping scientists and developers to
write code that shows better performance on irregular applications and at scale
when compared to more conventional programming models such as MPI. HPX-5
supports such applications with implementation of features like Active Global
Address Space (AGAS), ParalleX Processes, Complexes (ParalleX Threads and Thread
Management), Parcel Transport and Parcel Management, Local Control Objects
(LCOs) and Localities. 

General Information
--------------------

This document contains the release notes for the HPX–5 runtime system for 
exascale computing, release 1.1.0. Here we describe the status of HPX–5, 
including major improvements from the previous release, bug fixes and known 
limitations. All HPX–5 releases may be downloaded from the HPX–5 releases
web site. If you plan to use HPX–5, we suggest to start with the latest 
released version (currently HPX–5 v1.1.0) which can be downloaded from 
https://hpx.crest.iu.edu/download.

The documentation for the latest release of HPX–5 (currently v1.1.0) can
be found at https://hpx.crest.iu.edu/documentation. You can access versions 
of these documents specific to this release by going into the documentation
to the 1.1.0 release.

If you would like to work with the cutting edge version of HPX–5, we suggest
following the current health status of the develop branch at 
https://gitlab.crest.iu.edu/extreme/hpx. While we try to keep the develop 
branch stable and usable, sometimes new bugs trick their way into the code 
base - be aware!

The FAQ's for HPX–5 can be found at https://hpx.crest.iu.edu/faqs_and_tutorials.

Contents
-----------

The HPX-5 distribution contains two top-level directories.

* `/hpx`

This contains the HPX-5 interface and runtime library implementation, along with
examples, the test suite, HPX developer tutorials, and third-party dependencies
required for the build. The primary installed products from this project are the
HPX-5 headers and the `libhpx` runtime library, along with some library
dependencies and a `pkg-config` file for external projects.

* `/hpx-apps`

This contains example applications designed to depend on an externally installed
instance of HPX-5, and should serve as a usage model. In particular, it includes
a full application writing tutorial as well as the Lulesh and SSSP applications.

Features
-----------

The HPX-5 programming interface is designed around a cooperative lightweight
thread scheduler and unified access to a global address space. Event driven
programs invoke remote actions on global addresses using HPX-5's parcel
active-message plus continuation abstraction, or read or write global data
directly through one-sided network operations. Globally adressable lightweight
control objects (LCOs) provide control and data synchronization, allowing thread
execution or parcel instantiation to wait for events without execution resource
consumption. Finally, HPX-5's implementation of ParalleX processes provide
programmers the powerful abstraction of termination groups for parcel and thread
execution.

In addition to this core programming model HPX-5 provides a number of higher
level abstractions including asynchronous remote-procedure-call options, data
parallel loop constructs, global memory allocation, and system abstractions like
timers.

The HPX-5 C library implementation, libhpx, provides a high-performance,
portable implementation of the API that runs on both SMP and distributed
systems.

* Global Address Space

HPX-5 ships with two versions of the global address space. A traditional,
high-performance PGAS address space implements low-level one-sided and two-sided
(active message) access, while an experimental Active GAS implements two-sided
access while allowing the binding of global to physical addresses to vary
dynamically.

Memory management within the global address space is supported using the
high-performance jemalloc and tbbmalloc allocator.

* Lightweight Thread Scheduling

The HPX-5 lightweight threading system strives to minimize the cost of thread
creation and context switching to enable scalable massively threaded
computation. HPX-5 threads have unrestricted behavior and can block waiting for
asynchronous events by using lightweight control objects. Thread scheduling is
performed via local workstealing. Overall thread performance rivals other modern
packages with compatible semantics. The lightweight threading package is
currently compatible with x86_64 and ARM architectures.

* Parcels

HPX-5 parcels encode event-driven execution by specifying a traditional active
message and a continuation, allowing complex chains of computation to be
described. Parcels are sent to operate on addresses within the global address
space and model two-sided network programming. Parcel instantiation is
isomorphic with thread creation, in fact the parcel structure and thread control
block are the same structure.

* Lightweight Control Objects (LCOs)

LCOs represent control state in memory. Both threads and parcels interact with
LCOs, threads may block until an LCO is ready and/or its data is available,
while parcel send operations may wait until an LCO is ready. HPX-5 is
distributed with a number of built-in LCOs, including futures, and gates,
generation counters, parameterizable commutative-associative reductions,
semaphores, etc., and supports customizable user-defined LCOs as well.

* Processes

HPX-5 groups parcels and threads into processes, which may optionally have
termination detection enabled. Termination detection triggers an event when the
processed has reached quiescence. This powerful mechanism can be useful in
algorithms that do not have an easily described "join" point.

* Networking

In addition to active message parcel operations, HPX-5 provides direct access to
the global address space through standard one-sided operations. This support is
built on a novel interface to its networking layer,
put-with-remote-notification. HPX-5 provides two distinct implementation of this
abstraction, the PWC network layer on top of the Photon networking library which
implements put-with-remote-completion directly using rDMA, and the Isend/Irecv
networking later which emulates put-with-remote-completion using a traditional
MPI point-to-point implementation.

* Convenience Features

Development of HPX-5 was guided by codesign with computational scientists. This
resulted in a number of convenience features that encapsulate lower level
behavior in more convenient an familiar forms. These include a suite of
remote-procedure style calls, data parallel loops for local and distributed
data, and numerous smaller features.

3rd Party Dependencies Distributed With HPX-5 1.1.0
---------------------------------------------------

These packages are used in HPX-5 and included in its distribution.

* hwloc [http://www.open-mpi.org/projects/hwloc]

OpenMPI's hwloc infrastructure allows us to inspect the local node topology. 

* jemalloc [https://github.com/jemalloc/jemalloc]

The jemalloc scalable allocator provides a fast concurrent malloc
implementation, as well as an extended interface. HPX-5 relies on this extended
interface to manage its global address space, as well as registered memory where
appropriate.

* libffi [https://sourceware.org/libffi]

HPX-5 is a C library interface that was codesigned with computational
scientists. The libffi foreign function interface allows HPX-5 application
programmers to perform local and remote procedure calls using a dynamically
typed interface. This allows enables the remote-procedure-call interface, as
well as providing some type safety for calling otherwise untyped functions.

* photon

Photon is a low-level networking library that is co-developed with HPX-5. It
focuses on low overhead, high bandwidth, scalable operation and is implemented
directly on top of a number of different RDMA network platforms.

Photon provides a wide set of primitives, but provides HPX-5 with two primary
novel operations, put-with-command and get-with-command which extend traditional
RDMA with remote completion. In addition, photon provides an Atomic ops
interface, traditional RDMA, and two-sided point-to-point Isend/Irecv emulation.

* libsync (co-developed with HPX-5)

libsync is a small concurrent data structure library used by HPX-5 to support
threaded localities.

* libcuckoo [https://github.com/efficient/libcuckoo]
* CityHash [https://code.google.com/p/cityhash]

The experimental HPX-5 AGAS implementation relies on this concurrent allocator
to provide for global to physical translation and other block mapping needs.

Applications
--------------

The following are the apps currently released with HPX-5.

* LULESH

Proxy applications representing kernels of scientific computation toolkits have
been created and implemented in multiple programming models by the Department of
Energy's co-design centers, in part, to directly compare performance
characteristics and to explore and quantify their benefits. One of these proxy
applications is LULESH (Livermore Unstructured Lagrangian Explicit Shock
Hydrodynamics) [1], which has already played a key role in comparing and
contrasting programmability and performance among key programming models. In
Karlin et al.[2], a comparison of LULESH among OpenMP, MPI, MPI+OpenMP, CUDA,
Chapel, Charm++, Liszt, and Loci was presented with a focus on programmer
productivity, performance, and ease of optimizations. HPX-5 has also implemented
LULESH help further comparisons between these programming models. 

The HPX-5 version of LULESH comes in two flavors. The first, known as the
parcels implementation, does not take advantage of parallel for loop
opportunities and is more directly comparable to the MPI only implementation of
LULESH. The other flavor, known as the omp_parcels version, does take advantage
of parallel for loops and is more comparable to the MPI-OpenMP implementation of
LULESH. 

* LibPXGL

LibPXGL is the beginning of an endeavour to develop a next-generation graph
library based on HPX-5. Our first effort is invested in implementing graph
algorithms for solving the Single Source Shortest Path(SSSP) problem as SSSP is
a good representative of a class of irregular graph problems. The libPXGL SSSP
algorithms come in four flavors: chaotic algorithm, Delta-stepping algorithm,
Distributed Control,  k-level asynchronous algorithm. 

More details about applications can be found in
https://hpx.crest.iu.edu/applications.

Build Instructions
-------------------

The detailed build instructions can be found at
[https://hpx.crest.iu.edu/faqs_and_tutorials] or
[https://hpx.crest.iu.edu//Users_Guide]. 

What's new in HPX–5 v1.1.0?
----------------------------
List and brief how these changes will affect the user.

1. We have updated the configuration infrastructure. All optional functionality
is now enabled through --enable-opt options, while dependencies are specified 
through --with-dep options. Some functionality is directly mapped to a 
dependency (e.g., --enable-mpi and --with-mpi). Dependency specification has 
been extended, see configure --help for details.

2. We have updated to a more recent version of jemalloc. This has two benefits, 
the first is a reduction in configuration and build time because we only need 
one instance of the library, rather than up-to 3. The second is that we will 
be able to use an installed jemalloc 4.0 once it becomes available.

3. We have implemented support for GAS allocation using TBBMalloc. To use 
TBBMalloc add --disable-jemalloc --enable-tbbmalloc in the configure option and 
add intel module. TBBMalloc support requires a C++11 compiler.

4. We have implemented stack caching. HPX–5 applications malloc and free stacks
with high frequency. We previously relied on jemalloc memory caching in order 
to do this efficiently, however changes to jmealloc have reduced its performance
for larger object sizes. Now, each worker thread keeps a freelist of stacks 
and uses that. This has improved performance of fibonacci dramatically. By 
default we cache up to 32 stacks—this parameter can be set at runtime using the
--hpx-sched-stackcachelimit option; -1 for unbounded, 0 for no caching, and n 
for n stacks. We have noticed no downsides to caching and recommend it for all 
users.

5. Users can now print full configuration using --hpx-log-level=config.

6. Users can now control SMP optimization with --hpx-opt-smp.

7. Users can print the version information at runtime with --hpx-version.

8. Various changes and improvements have been made to the instrumentation 
system: The amount of data recorded when tracing has been reduced considerably. 
New events and event classes have been added, including a new class of events 
for LCO events and new class of events for HPX–5 process events. Events were 
added for threads being suspended and resumed. Some improvements were made to
the data recorded for some existing events. Finally, tracing now generates a table 
mapping action ids to action names, to make parcel events easier to use.

API Changes: Improvements/Enhancements
----------------------------------------
In this release we have made several significant changes:

In this release, we resolved the inconsistency between the interface to pass 
data to continuations (hpx_thread_continue), and the interface to pass data to 
an RPC invocation (hpx_call). We made hpx_thread_continue variadic such that it 
supports the same convention: For example, it is possible to now use 
hpx_thread_continue(&arg1, &arg2, ..., &argN) to continue data to a default 
action, and use hpx_thread_continue(&arg, sizeof(arg)) to continue data to a 
marshalled action. This API change will silently break existing apps unless 
the order of the arguments to hpx_thread_continue() are swapped.

Users may programmatically query the libhpx configuration by including 
libhpx/libhpx.h and using the libhpx_get_config() function and libhpx_config_t 
interface.

Bug Fixes
-------------
1. Generation counters fail for ninplace > 0.

2. The command line arguments enabling tracing were fixed so that the 
"all" argument is respected.

3. Enabling tracing no longer causes the working directory to be changed.

4. Unaligned memgets on ugni will generate an alignment error.

5. Memory corruption with photon/jemalloc.

Known Bugs & Limitations
-------------------------

1. AGAS and TBBMalloc require a C++11 compiler.

2. The AGAS implementation only works with the Isend/Irecv network, and is 
still considered experimental.

3. Clang versions prior to 3.6 (.e., 3.5.1 and earlier) contain regressions 
that trigger runtime failures in HPX.

4. Cray support is experimental (use aprun -n [nodes] -N 1 -d [ppn] ./app 
--hpx-threads=[ppn]). The -d option (or equivalent) is necessary to correctly 
distribute worker threads, and the --hpx-threads option must be specified explicitly.

5. Building with both --enable-mpi and --enable-pmi on Edison makes cray-mpich 
terminate poorly (Isend/Irecv).

6. HPX–5 fails to compile some examples when using Cray Compilers (CrayCC).

7. Delta-stepping and KLA algorithms crashing with photon for large scale with
jemalloc and without jemalloc.

8. HPX–5 only runs with 1 thread by default on Jetson boards.

9. NORETURN lightweight thread termination does not run C++ destructors.

Acknowledgements
-----------------
We would like to acknowledge the DoD, DoE, NSF, PSAAP, XPRESS, XSEDE who fund
and support our work. HPX-5 is currently funded by:

1. Advanced Development of the DoD Extreme-scale Execution Framework
   Special Project BY11-034SP: Advanced Development of the DoD Extreme-scale
   Execution Framework. Acknowledgment: no specific clause is needed;
   subcontractor must indicate “PO #190, Task Order #002, Project #BY11-034SP”
   on all invoices and other communications for work performed under the terms
   of this subcontract.
   
2. DASHMM: Award Abstract #1440396 SI2-SSE: Dynamic Adaptive Runtime Systems for
   Advanced Multipole Method-based Science Achievement.
   
3. Photon: Photon is funded from National Science Foundation, OCI-1238379.

4. PSAAP: “This material is based upon work supported by the Department of
   Energy, National Nuclear Security Administration, under Award Number(s)
   DE-NA0002377.” The Centers involved include: Center for Research in Extreme
   Scale Technologies (CREST), Indiana University Center for Shock-Wave
   Processing of Advanced Reactive Materials, University of Notre Dame PXGL:
   “This material is based upon work supported by the National Science
   Foundation under Grant No. 1111888.”
   
5. PXGL: “This material is based upon work supported by the National Science
   Foundation under Grant No. 1111888.”
   
6. XPRESS: eXascale Programming Environment and System Software
   XPRESS Title: eXascale PRogramming Environment and System Software (XPRESS)
   Acknowledgment: “This material is based upon work supported by the Department
   of Energy under Award Number(s) DE-SC0008809.” Disclaimer: “This report was
   prepared as an account of work sponsored by an agency of the United States
   Government. Neither the United States Government nor any agency thereof, nor
   any of their employees, makes any warranty, express or implied, or assumes
   any legal liability or responsibility for the accuracy, completeness, or
   usefulness of any information, apparatus, product, or process disclosed, or
   represents that its use would not infringe privately owned rights. Reference
   herein to any specific commercial product, process, or service by trade name,
   trademark, manufacturer, or otherwise does not necessarily constitute or
   imply its endorsement, recommendation, or favoring by the United States
   Government or any agency thereof. The views and opinions of authors expressed
   herein do not necessarily state or reflect those of the United States
   Government or any agency thereof.”
   
7. XSEDE computer time grant TG-CCR140035 “Dynamic Introspective Runtime to
   Enable Extreme Scaling for Irregular Time-varying Problems”. This work used
   the Extreme Science and Engineering Discovery Environment (XSEDE), which is
   supported by National Science Foundation grant number #ACI-1053575.
